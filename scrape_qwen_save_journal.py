import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

from typing import *

import os
import json

from openai import OpenAI
from openai.types.chat.chat_completion import Choice

from rss import get_all_journal_articles
import re
import feedparser

from datetime import datetime, timedelta

from config import FIELD_KEYWORDS, JOURNALS_CONFIG, get_db_connection, init_database
from mysql.connector import Error


# é…ç½®ä¿¡æ¯
CLIENT = OpenAI(
    api_key="sk-dc79c7928859459c9619daf752c542fc",  # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…APIå¯†é’¥
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

def identify_journal_from_link(link):
    """æ ¹æ®é“¾æ¥è¯†åˆ«æœŸåˆŠï¼Œä½¿ç”¨é…ç½®ä¸­çš„domain_patterns"""
    if not link or link == 'æ— é“¾æ¥':
        return 'æœªçŸ¥æœŸåˆŠ'
        
    # å…ˆæ£€æŸ¥ç°æœ‰æœŸåˆŠ
    for journal_id, config in JOURNALS_CONFIG.items():
        domain_patterns = config.get('domain_patterns', [])
        exclude_patterns = config.get('exclude_patterns', [])
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…domain_patternsä¸­çš„ä»»ä¸€æ¨¡å¼
        if any(pattern in link for pattern in domain_patterns):
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…exclude_patternsä¸­çš„ä»»ä¸€æ¨¡å¼
            if not any(ex_pattern in link for ex_pattern in exclude_patterns):
                return config.get('journal_name', journal_id)
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æœŸåˆŠ
    return 'æœªçŸ¥æœŸåˆŠ'

def get_detailed_abstract(entry):
    """é›†æˆæœç´¢ä¸ç¿»è¯‘ï¼ˆä¿®å¤å­—æ®µåˆå¹¶é—®é¢˜ï¼‰"""
    # ä»é“¾æ¥è§£ææœŸåˆŠåç§°
    link = getattr(entry, 'link', '')
    
    # å¦‚æœentryå·²ç»æœ‰journalå±æ€§ï¼Œä¼˜å…ˆä½¿ç”¨
    journal = getattr(entry, 'journal', None)
    if not journal:
        journal = identify_journal_from_link(link)
        
    # è¡¥å…¨entryçš„journalå±æ€§
    entry.journal = journal
    # åŸºç¡€å…ƒæ•°æ®æå–ï¼ˆå¿…é¡»å­—æ®µï¼‰
    base_data = {
        'journal': getattr(entry, 'journal', 'æœªçŸ¥æœŸåˆŠ'),
        'original_title': getattr(entry, 'title', 'æ— æ ‡é¢˜'),
        'original_authors': ', '.join(a.name for a in getattr(entry, 'authors', [])),
        'link': getattr(entry, 'link', 'æ— é“¾æ¥'),
        'publish_date': parse_entry_date(entry).strftime('%Y-%m-%d')
    }

    try:
        # å¢å¼ºç‰ˆç³»ç»Ÿæç¤º
        system_prompt = """ä½œä¸ºå­¦æœ¯åŠ©æ‰‹ï¼Œè¯·å®Œæˆï¼š
1. ç½‘ç»œæœç´¢è·å–ç”¨æˆ·æåˆ°çš„è®ºæ–‡æ‘˜è¦ï¼ˆ250å­—ä»¥å†…ï¼Œä¸­æ–‡ï¼‰
2. ç¿»è¯‘æ ‡é¢˜å’Œä½œè€…ä¿¡æ¯
3. jsonç»“æ„åŒ–è¿”å›ï¼š
{
  "original_title": "ä¿ç•™åŸå§‹æ ‡é¢˜",
  "translated_title": "ä¸­æ–‡æ ‡é¢˜",
  "original_authors": "åŸå§‹ä½œè€…åˆ—è¡¨", 
  "translated_authors": "ä¸­æ–‡ä½œè€…åˆ—è¡¨",
  "abstract": "ä¸­æ–‡æ‘˜è¦",
  "summary": "100å­—æ€»ç»“"
}"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¯·å¤„ç†ï¼š{base_data['original_title']}"}
        ]

        # è°ƒç”¨é€šä¹‰åƒé—®APIè¿›è¡Œè”ç½‘æœç´¢è·å–ä¿¡æ¯
        completion = CLIENT.chat.completions.create(
            model="qwen-plus",  # é€šä¹‰åƒé—®æ¨¡å‹
            messages=messages,
            temperature=0.3,
            response_format={"type": "json_object"},
            extra_body={
                "enable_search": True  # å¯ç”¨è”ç½‘æœç´¢
            }
        )

        # è§£æç»“æ„åŒ–å“åº”
        try:
            response = completion.choices[0].message.content
            # æ·»åŠ å®‰å…¨çš„JSONè§£æ
            api_data = json.loads(response)  # ä¸éœ€è¦æŒ‡å®šencoding
            
            # åˆå¹¶æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨APIæ•°æ®ï¼Œä¿ç•™åŸºç¡€å…ƒæ•°æ®ï¼‰
            return {
                **base_data,  # åŸºç¡€å­—æ®µ
                'translated_title': api_data.get('translated_title', base_data['original_title']),
                'translated_authors': api_data.get('translated_authors', base_data['original_authors']),
                'abstract': api_data.get('abstract', 'æ‘˜è¦è·å–å¤±è´¥'),
                'summary': api_data.get('summary', 'æ€»ç»“ç”Ÿæˆå¤±è´¥')
            }
        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€æ•°æ®+é”™è¯¯ä¿¡æ¯
            return {
                **base_data,
                'translated_title': base_data['original_title'],
                'translated_authors': base_data['original_authors'],
                'abstract': 'å†…å®¹å¤„ç†å¼‚å¸¸',
                'summary': 'å†…å®¹å¤„ç†å¼‚å¸¸'
            }

    except Exception as e:
        print(f"âš ï¸ å…¨å±€å¼‚å¸¸: {str(e)}")
        return {
            'Translated Title': base_data['original_title'],
            'Translated Authors': base_data['original_authors'],
            'Abstract': 'æœåŠ¡æš‚æ—¶ä¸å¯ç”¨'
        }

def classify_article_fields(article_data: dict) -> Dict[str, float]:
    """åˆ¤æ–­æ–‡ç« å±äºå“ªäº›é¢†åŸŸï¼Œè¿”å›é¢†åŸŸåŠç½®ä¿¡åº¦"""
    title = article_data.get('original_title', 'æ— æ ‡é¢˜')[:50]
    print(f"ğŸ” æ­£åœ¨åˆ¤æ–­ã€Š{title}...ã€‹å±äºå“ªäº›é¢†åŸŸ...")
    
    # å‡†å¤‡é¢†åŸŸåˆ—è¡¨ï¼Œæ’é™¤"æ— "
    fields = [field for field in FIELD_KEYWORDS.keys() if field != "æ— "]
    
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©æ‰‹ï¼Œéœ€è¦åˆ¤æ–­è®ºæ–‡å±äºå“ªäº›ç ”ç©¶é¢†åŸŸã€‚

è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{article_data.get('original_title', '')}
æ‘˜è¦ï¼š{article_data.get('abstract', '')[:2000]}

è¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡å±äºä»¥ä¸‹å“ªäº›é¢†åŸŸï¼ˆå¯å¤šé€‰ï¼‰ï¼Œå¹¶ç»™å‡ºç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„å°æ•°ï¼‰ï¼š
{', '.join(fields)}

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
  "fields": {{
    "é¢†åŸŸ1": 0.95,
    "é¢†åŸŸ2": 0.85,
    "é¢†åŸŸ3": 0.75
  }}
}}

åªè¿”å›ç½®ä¿¡åº¦å¤§äº0.5çš„é¢†åŸŸã€‚"""

    try:
        completion = CLIENT.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "è¯·åˆ†æè¿™ç¯‡è®ºæ–‡å±äºå“ªäº›é¢†åŸŸ"}
            ],
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(completion.choices[0].message.content)
        field_results = result.get("fields", {})
        
        # æ‰“å°ç»“æœ
        if field_results:
            print("âœ… é¢†åŸŸåˆ†ç±»ç»“æœ:")
            for field, confidence in field_results.items():
                print(f"  - {field}: {confidence:.2f}")
        else:
            print("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„é¢†åŸŸ")
            
        return field_results
        
    except Exception as e:
        print(f"âš ï¸ é¢†åŸŸåˆ¤æ–­å¤±è´¥: {str(e)}")
        return {}  # å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸

def save_to_database(df):
    """å¢å¼ºå­—æ®µéªŒè¯å¹¶ä¿å­˜æ–‡ç« åŠå…¶é¢†åŸŸåˆ†ç±»"""
    # åˆæ³•å­—æ®µåˆ—è¡¨
    valid_columns = {
        'journal', 'original_title', 'translated_title',
        'original_authors', 'translated_authors',
        'abstract', 'summary', 'link', 'publish_date'
    }
    
    # è¿‡æ»¤éæ³•å­—æ®µ
    df = df[[col for col in df.columns if col in valid_columns]]
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # è·å–å½“å‰è®°å½•æ•°
        cursor.execute("SELECT COUNT(*) FROM articles")
        original_count = cursor.fetchone()[0]
        
        # ä¿å­˜æ–‡ç« å¹¶è·å–æ–°å¢æ–‡ç« çš„ID
        new_article_ids = []
        
        for _, row in df.iterrows():
            # å‡†å¤‡æ’å…¥æ•°æ®
            columns = ', '.join(row.index)
            placeholders = ', '.join(['%s'] * len(row))
            
            # æ‰§è¡Œæ’å…¥
            cursor.execute(f"""
                INSERT IGNORE INTO articles 
                ({columns}) 
                VALUES ({placeholders})
            """, tuple(row))
            
            # å¦‚æœæ˜¯æ–°æ’å…¥çš„æ–‡ç« ï¼Œè·å–å…¶ID
            if cursor.rowcount > 0:
                article_id = cursor.lastrowid
                new_article_ids.append((article_id, row.to_dict()))
        
        conn.commit()
        
        # è·å–ä¿å­˜åçš„è®°å½•æ•°
        cursor.execute("SELECT COUNT(*) FROM articles")
        new_count = cursor.fetchone()[0]
        added = new_count - original_count
        skipped = len(df) - added
        
        print(f"ğŸ’¾ æ•°æ®åº“æ›´æ–°: æ–°å¢ {added} ç¯‡, è·³è¿‡ {skipped} ç¯‡é‡å¤")
        
        # ä¸ºæ–°å¢æ–‡ç« è¿›è¡Œé¢†åŸŸåˆ†ç±»å¹¶ä¿å­˜
        for article_id, article_data in new_article_ids:
            print(f"\nğŸ” ä¸ºæ–‡ç« ID {article_id} è¿›è¡Œé¢†åŸŸåˆ†ç±»...")
            field_results = classify_article_fields(article_data)
            
            # ä¿å­˜é¢†åŸŸåˆ†ç±»ç»“æœ
            for field, confidence in field_results.items():
                cursor.execute("""
                    INSERT INTO article_fields 
                    (article_id, field, confidence) 
                    VALUES (%s, %s, %s)
                    ON DUPLICATE KEY UPDATE confidence = VALUES(confidence)
                """, (article_id, field, confidence))
            
            conn.commit()
            print(f"âœ… å·²ä¿å­˜ {len(field_results)} ä¸ªé¢†åŸŸåˆ†ç±»ç»“æœ")
            
            # å¦‚æœå¯ç”¨äº†é€»è¾‘å…³ç³»å›¾åŠŸèƒ½ï¼Œä¸ºæ–°æ–‡ç« ç”Ÿæˆé€»è¾‘å…³ç³»å›¾
            if LOGIC_GRAPH_ENABLED:
                if article_data.get('abstract') and article_data.get('abstract') not in ['æ‘˜è¦è·å–å¤±è´¥', 'å†…å®¹å¤„ç†å¼‚å¸¸']:
                    print(f"ğŸ”„ ä¸ºæ–‡ç« ID {article_id}ã€Œ{article_data.get('translated_title', '')[:20]}...ã€ç”Ÿæˆé€»è¾‘å…³ç³»å›¾...")
                    try:
                        start_time = time.time()
                        result = process_article_logic_graph(article_id)
                        elapsed = time.time() - start_time
                        
                        if result:
                            print(f"âœ… é€»è¾‘å…³ç³»å›¾ç”ŸæˆæˆåŠŸ (è€—æ—¶: {elapsed:.1f}ç§’)")
                            # æ˜¾ç¤ºç”Ÿæˆçš„mermaidä»£ç é•¿åº¦ä½œä¸ºå‚è€ƒ
                            mermaid_length = len(result.get('mermaid_code', ''))
                            print(f"   mermaidä»£ç é•¿åº¦: {mermaid_length} å­—ç¬¦")
                        else:
                            print(f"âš ï¸ é€»è¾‘å…³ç³»å›¾ç”Ÿæˆå¤±è´¥ (è€—æ—¶: {elapsed:.1f}ç§’)")
                    except Exception as e:
                        print(f"âŒ é€»è¾‘å…³ç³»å›¾å¤„ç†å¼‚å¸¸: {str(e)}")
                else:
                    print(f"â© æ–‡ç« ID {article_id} æ‘˜è¦ä¸å¯ç”¨ï¼Œè·³è¿‡é€»è¾‘å…³ç³»å›¾ç”Ÿæˆ")
            
            print(f"âœ“ æ–‡ç« ID {article_id} å¤„ç†å®Œæˆ\n{'='*50}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
        raise
    finally:
        if 'conn' in locals() and conn.is_connected():
            cursor.close()
            conn.close()

def article_exists(link: str, entry) -> bool:
    """å¢å¼ºå»é‡æ£€æŸ¥"""
    if not link or link == 'æ— é“¾æ¥':
        return False
        
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        "SELECT 1 FROM articles WHERE link = %s OR (journal = %s AND original_title = %s) LIMIT 1",
        (link, getattr(entry, 'journal', 'æœªçŸ¥æœŸåˆŠ'), getattr(entry, 'title', 'æ— æ ‡é¢˜'))
    )
    result = cursor.fetchone()
    cursor.close()
    conn.close()
    return result is not None

def get_rss_articles(articles_num=3):
    """è·å–å¹¶å¤„ç†æœŸåˆŠæ–‡ç« ï¼ˆå¢å¼ºæ—¥å¿—è¾“å‡ºï¼‰"""
    print(f"\nğŸ” å¼€å§‹è·å–æœ€æ–°æ–‡ç« ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M')}ï¼‰")
    all_articles = []
    
    entries = get_all_journal_articles(articles_num)
    total = len(entries)
    print(f"ğŸ“¥ å…±å‘ç° {total} ç¯‡å€™é€‰æ–‡ç« ")
    
    # æ–°å¢å­—å…¸æ¥å­˜å‚¨æ¯ä¸ªæœŸåˆŠçš„æ ‡é¢˜å’Œæ•°é‡
    journal_count = {}

    for idx, entry in enumerate(entries, 1):
        try:
            # è¿›åº¦æ˜¾ç¤º
            progress = f"[{idx}/{total}]"
            
            # åŸºç¡€ä¿¡æ¯
            title = getattr(entry, 'title', 'æ— æ ‡é¢˜')[:40] + "..." 
            link = getattr(entry, 'link', 'æ— é“¾æ¥')
            journal = getattr(entry, 'journal', 'æœªçŸ¥æœŸåˆŠ')  # è·å–æœŸåˆŠåç§°
            print(f"\n{progress} ğŸ“„ å¤„ç†æ–‡ç« : {title}")
            print(f"ğŸ”— æ–‡ç« é“¾æ¥: {link}")
            
            # æ£€æŸ¥é‡å¤
            if article_exists(link, entry):
                print("â© è·³è¿‡é‡å¤æ–‡ç« ")
                continue
                
            # å¤„ç†æµç¨‹
            start_time = time.time()
            processed_data = get_detailed_abstract(entry)
            elapsed = time.time() - start_time
            
            if processed_data:
                print(f"âœ… å¤„ç†æˆåŠŸï¼ˆè€—æ—¶{elapsed:.1f}sï¼‰")
                # æ˜¾ç¤ºæ–‡ç« åŸºæœ¬ä¿¡æ¯
                print(f"  æœŸåˆŠ: {processed_data.get('journal', 'æœªçŸ¥')}")
                print(f"  æ ‡é¢˜: {processed_data.get('translated_title', 'æ— æ ‡é¢˜')[:50]}...")
                df = pd.DataFrame([processed_data])
                save_to_database(df)
                all_articles.append(processed_data)

                # æ›´æ–°æœŸåˆŠè®¡æ•°
                if journal in journal_count:
                    journal_count[journal] += 1
                else:
                    journal_count[journal] = 1
            else:
                print("âš ï¸ å¤„ç†å¤±è´¥ï¼Œä¿ç•™åŸºç¡€ä¿¡æ¯")
                all_articles.append({
                    'original_title': title,
                    'link': link,
                    'publish_date': datetime.now().strftime('%Y-%m-%d')
                })
                
        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯: {str(e)}")
            continue
            
    print(f"\nğŸ“Š æœ€ç»ˆå¤„ç†å®Œæˆï¼šæˆåŠŸ {len(all_articles)} ç¯‡ï¼Œå¤±è´¥ {total - len(all_articles)} ç¯‡")
    
    # è¾“å‡ºæ¯ä¸ªæœŸåˆŠè·å–åˆ°çš„æ•°æ®æ¡æ•°å’Œå¯¹åº”æ ‡é¢˜
    print("\nğŸ“š å„æœŸåˆŠè·å–åˆ°çš„æ•°æ®æ¡æ•°ï¼š")
    for journal, count in journal_count.items():
        print(f"{journal}: {count} æ¡")

    return pd.DataFrame(all_articles)

def parse_entry_date(entry):
    """å¢å¼ºæ—¥æœŸè§£æå¯é æ€§"""
    try:
        # ä¼˜å…ˆä½¿ç”¨å·²è§£æçš„æ—¥æœŸ
        if hasattr(entry, 'published_parsed'):
            return datetime(*entry.published_parsed[:6])
            
        # å¤‡é€‰æ—¥æœŸå­—æ®µ
        date_str = getattr(entry, 'published', '') or \
                  getattr(entry, 'updated', '') or \
                  getattr(entry, 'created', '')
                  
        if not date_str:
            return datetime.now()
            
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        for fmt in ('%a, %d %b %Y %H:%M:%S %Z',
                   '%Y-%m-%dT%H:%M:%SZ',
                   '%Y-%m-%d %H:%M:%S',
                   '%Y-%m-%d'):
            try:
                return datetime.strptime(date_str, fmt)
            except:
                continue
        return datetime.now()
    except:
        return datetime.now()

def is_related_to_field(article_data: dict, target_field: str) -> bool:
    """åˆ¤æ–­æ–‡ç« æ˜¯å¦å±äºç‰¹å®šé¢†åŸŸï¼ˆä»æ•°æ®åº“æŸ¥è¯¢ï¼‰"""
    # è·å–æ–‡ç« ID
    article_id = get_article_id(article_data.get('link', ''))
    if not article_id:
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ç« IDï¼Œä½¿ç”¨æ—§æ–¹æ³•è¿›è¡Œåˆ¤æ–­
        return legacy_is_related_to_field(article_data, target_field)
    
    # ä»æ•°æ®åº“æŸ¥è¯¢
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("""
        SELECT confidence FROM article_fields 
        WHERE article_id = %s AND field = %s
    """, (article_id, target_field))
    result = cursor.fetchone()
    cursor.close()
    conn.close()
    
    if result:
        confidence = result[0]
        print(f"ğŸ” æ–‡ç« ä¸é¢†åŸŸ {target_field} çš„ç›¸å…³åº¦: {confidence:.2f}")
        return confidence > 0.5
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç« ä¸é¢†åŸŸ {target_field} çš„å…³è”è®°å½•")
        return False

def get_article_id(link: str) -> int:
    """æ ¹æ®é“¾æ¥è·å–æ–‡ç« ID"""
    if not link:
        return None
        
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM articles WHERE link = %s", (link,))
    result = cursor.fetchone()
    cursor.close()
    conn.close()
    
    return result[0] if result else None

def legacy_is_related_to_field(article_data: dict, target_field: str) -> bool:
    """æ—§ç‰ˆé¢†åŸŸåˆ¤æ–­æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
    title = article_data.get('original_title', 'æ— æ ‡é¢˜')[:50]
    print(f"ğŸ” æ­£åœ¨åˆ¤æ–­ã€Š{title}...ã€‹æ˜¯å¦å±äº {target_field} é¢†åŸŸ...", end='')
    
    system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”åŠ©æ‰‹ï¼Œéœ€è¦åˆ¤æ–­è®ºæ–‡æ˜¯å¦å±äºç‰¹å®šç ”ç©¶é¢†åŸŸã€‚
    
ç›®æ ‡é¢†åŸŸï¼š{target_field}
è®ºæ–‡ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{article_data.get('original_title', '')}
æ‘˜è¦ï¼š{article_data.get('abstract', '')[:2000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…token

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™åˆ¤æ–­ï¼š
1. å¦‚æœè®ºæ–‡æ˜æ˜¾å±äº{target_field}é¢†åŸŸï¼Œè¿”å›Y
2. å¦‚æœä¸ç¡®å®šæˆ–ä¸å±äºï¼Œè¿”å›N
3. åªéœ€è¿”å›å•ä¸ªå­—æ¯ï¼Œä¸è¦ä»»ä½•è§£é‡Š"""

    try:
        completion = CLIENT.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "è¯·åˆ¤æ–­è¿™ç¯‡è®ºæ–‡æ˜¯å¦å±äºç›®æ ‡é¢†åŸŸ"}
            ],
            temperature=0.1,
            max_tokens=1
        )
        result = completion.choices[0].message.content.strip().upper() == 'Y'
        print("âœ… å±äº" if result else "âŒ ä¸å±äº")
        return result
        
    except Exception as e:
        print(f"âš ï¸ åˆ¤æ–­å¤±è´¥: {str(e)}")
        return False  # å¤±è´¥æ—¶é»˜è®¤ä¿ç•™

def check_database_exists():
    """æ£€æŸ¥äº‘ç«¯æ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
    try:
        conn = get_db_connection()  # ä½¿ç”¨ä½ ä¹‹å‰å®šä¹‰çš„è·å–æ•°æ®åº“è¿æ¥çš„å‡½æ•°
        cursor = conn.cursor()
        
        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        cursor.execute("SHOW DATABASES LIKE 'academic_pulse';")
        result = cursor.fetchone()
        print(result)
        if result:
            print("âœ… æ•°æ®åº“ 'academic_pulse' å·²å­˜åœ¨ã€‚")
        else:
            print("âŒ æ•°æ®åº“ 'academic_pulse' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
            init_database()  # è°ƒç”¨åˆå§‹åŒ–å‡½æ•°åˆ›å»ºæ•°æ®åº“å’Œè¡¨ç»“æ„
        return(result)

    except Error as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
    finally:
        if 'conn' in locals() and conn.is_connected():
            cursor.close()
            conn.close()

if __name__ == "__main__":

    check_database_exists()  # æ£€æŸ¥äº‘ç«¯æ•°æ®åº“
    # ç¡®ä¿æ•°æ®åº“å­˜åœ¨

    # å¯¼å…¥é€»è¾‘å…³ç³»å›¾ç”Ÿæˆæ¨¡å—
    try:
        from generate_logic_graph import process_article_logic_graph, ensure_logic_graph_table_exists

        LOGIC_GRAPH_ENABLED = True
        print("âœ… é€»è¾‘å…³ç³»å›¾æ¨¡å—å·²åŠ è½½ï¼Œå°†è‡ªåŠ¨ä¸ºæ–°æ–‡ç« ç”Ÿæˆé€»è¾‘å…³ç³»å›¾")
    except ImportError:
        print("âš ï¸ é€»è¾‘å…³ç³»å›¾æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†ä¸ç”Ÿæˆé€»è¾‘å…³ç³»å›¾")
        LOGIC_GRAPH_ENABLED = False

    # å¦‚æœå¯ç”¨äº†é€»è¾‘å…³ç³»å›¾åŠŸèƒ½ï¼Œç¡®ä¿ç›¸å…³è¡¨å­˜åœ¨
    if LOGIC_GRAPH_ENABLED:
        print("ğŸ”„ ç¡®ä¿é€»è¾‘å…³ç³»å›¾è¡¨ç»“æ„å­˜åœ¨...")
        ensure_logic_graph_table_exists()
        
    # åªæ‰§è¡ŒæŠ“å–å’Œä¿å­˜
    print("\nğŸš€ å¼€å§‹æŠ“å–æœŸåˆŠæ–‡ç« å¹¶ç”Ÿæˆé€»è¾‘å…³ç³»å›¾...")
    get_rss_articles()
    print("âœ… æ•°æ®æŠ“å–ã€ä¿å­˜å’Œé€»è¾‘å…³ç³»å›¾ç”Ÿæˆ å…¨éƒ¨å®Œæˆ")
